from langchain_ollama import ChatOllama
from langchain_neo4j import Neo4jGraph, GraphCypherQAChain
from langchain.schema import SystemMessage, HumanMessage
from langchain_community.tools.wikidata.tool import WikidataAPIWrapper, WikidataQueryRun
from langchain_ollama import ChatOllama
from langchain_community.graphs.rdf_graph import RdfGraph
from langchain_community.chains.graph_qa.sparql import GraphSparqlQAChain
from langchain_openai import ChatOpenAI
import logging

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)


# Configura la API de Ollama
llm_neo4j = ChatOpenAI(model="gpt-4-turbo", temperature=0, max_tokens=None, timeout=None, max_retries=2, base_url="https://openrouter.ai/api/v1",  api_key="API_KEY")
llm_wikidata = ChatOllama(model="llama3", base_url="http://ollama.gsi.upm.es")
llm_dbpedia = ChatOllama(model="mistral", base_url="https://ollama.gsi.upm.es/")
llm = ChatOllama(model="phi4:14b", base_url="https://ollama.gsi.upm.es/")

# Conexi√≥n a Neo4j a trav√©s de LangChain
graph_neo4j = Neo4jGraph(
    url="bolt://neo4j:7687",
    username="neo4j",
    password="password"
)

graph_dbpedia = RdfGraph(
    query_endpoint="https://dbpedia.org/sparql",
    standard="rdf"
)


def get_response_from_neo4j(pregunta):
    try:
        qa_chain = GraphCypherQAChain.from_llm(
            llm=llm_neo4j,
            graph=graph_neo4j,
            verbose=True,
            return_intermediate_steps=True,
            allow_dangerous_requests=True
        )

        respuesta = qa_chain.invoke({"query": pregunta})
        
        logger.info("üîç Respuesta completa de Neo4j QA Chain:")
        logger.info(respuesta)

        # Verificamos si el contexto est√° vac√≠o
        intermediate_steps = respuesta.get("intermediate_steps", [])
        if intermediate_steps and isinstance(intermediate_steps[1].get("context", None), list):
            if len(intermediate_steps[1]["context"]) == 0:
                logger.info("‚ö†Ô∏è No se encontraron resultados en Neo4j.")
                return None

        return respuesta["result"]

    except Exception as e:
        print(f"‚ùå Error al consultar Neo4j: {e}")
        return None



def get_response_from_wikidata(pregunta):
    # Obtener la entidad de Wikidata
    messages = [
        SystemMessage(content="""Eres un asistente que ayuda a identificar entidades espec√≠ficas para ser consultadas en Wikidata. 
        Dada una pregunta del usuario, devuelve el nombre COMPLETO Y EXACTO de la entidad principal a buscar en Wikidata.
        No devuelvas nada ambiguo. No expliques nada. Solo responde con el nombre m√°s apropiado para Wikidata."""),
        HumanMessage(content=pregunta)
    ]

    entidad = llm_wikidata.invoke(messages).content
    logger.info("Entidad de Wikidata: %s", entidad)

    # Usar Wikidata API para obtener informaci√≥n relevante
    wikidata = WikidataQueryRun(api_wrapper=WikidataAPIWrapper())
    respuesta_raw = wikidata.run(entidad)
    logger.info("Contexto Wikidata: %s", respuesta_raw)

    # Verifica si la respuesta es "No good Wikidata Search Result was found"
    if "No good Wikidata Search Result was found" in respuesta_raw:
        logger.warning("‚ö†Ô∏è No se encontr√≥ un buen resultado en Wikidata para la entidad: %s", entidad)
        return None  # Aqu√≠ no se retorna respuesta de Wikidata y podemos continuar con DBpedia o Neo4j

    # Usar el LLM para responder de forma natural con el contexto obtenido
    messages_final = [
        SystemMessage(content="""Eres un asistente que responde preguntas de los usuarios utilizando informaci√≥n estructurada de Wikidata. 
        A continuaci√≥n recibir√°s una pregunta y un contexto que contiene datos relevantes de Wikidata. 
        Responde de forma natural, clara y directa, utilizando solo la informaci√≥n del contexto. 
        Si no encuentras la respuesta, ind√≠calo."""), 
        HumanMessage(content=f"Pregunta: {pregunta}\n\nContexto:\n{respuesta_raw}")
    ]

    respuesta_final = llm_wikidata.invoke(messages_final).content

    return respuesta_final

def get_response_from_dbpedia(pregunta):
    logger.info("üåê [DBpedia] Consultando DBpedia con la pregunta: %s", pregunta)

    # Configurar el grafo RDF y el LLM
    chain = GraphSparqlQAChain.from_llm(
        llm=llm_dbpedia,  # reutiliza el ChatOllama ya instanciado globalmente
        graph=graph_dbpedia,
        allow_dangerous_requests=True,
        return_sparql_query=True,
    )

    # Bloque de namespaces para ayudar al modelo
    query = """
        Associated namespaces:
        dbr:  <http://dbpedia.org/resource/>
        dbo:  <http://dbpedia.org/ontology/>
        rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
    """

    # Pregunta del usuario (solo texto, sin necesidad de tocar el c√≥digo)
    user_question = pregunta

    # Concatenar la ayuda (namespaces) + pregunta del usuario
    full_prompt = f"{query}\n\n{user_question}"

    try:
        result = chain.invoke({"query": full_prompt}, return_only_outputs=False)
        logger.info("Contexto DBpedia: %s", result)
        respuesta = result["result"]
        if not respuesta:
            logger.warning("‚ö†Ô∏è DBpedia no devolvi√≥ respuesta √∫til.")
            return None
        return respuesta

    except ValueError as e:
        logger.error("‚ùå Error al ejecutar la consulta SPARQL: %s", e)
        return None
        
    except Exception as e:
        logger.exception("‚ùå Error inesperado al consultar DBpedia")
        return None



def get_response_from_llm(pregunta):
    logger.info("üí¨ [LLM] Generando respuesta con modelo local como √∫ltimo recurso...")    
    respuesta = llm.invoke(pregunta).content
    advertencia = "‚ö†Ô∏è *This response has been generated by AI without support from verified sources. Consider checking it against official sources.*\n\n"
    return advertencia + respuesta
